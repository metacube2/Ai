# Mail Fine-Tuning Web-App fÃ¼r macOS (Apple Silicon)

Eine vollstÃ¤ndige lokale Web-Anwendung fÃ¼r das Fine-Tuning von LLMs auf Mail-Daten, optimiert fÃ¼r Apple Silicon (M4 Pro mit 24GB RAM).

## Features

- ğŸ“¥ **Mail Import**: Drag & Drop Upload von .mbox, .eml, .txt Dateien mit automatischer Bereinigung
- ğŸ·ï¸ **Labeling Interface**: Komfortable UI zum manuellen Labeln von Mails
- ğŸ“Š **Export & Statistiken**: JSONL Export fÃ¼r Training mit detaillierten Statistiken
- ğŸ¤– **Modell-Management**: Verwaltung von MLX-Modellen
- ğŸ¯ **Training**: LoRA Fine-Tuning mit Live-Updates und Visualisierung
- ğŸ§ª **Evaluation**: Chat-Interface mit Vergleichsmodus (Base vs. Fine-tuned)

## Technologie-Stack

- **Backend**: Python (FastAPI)
- **Frontend**: HTML/CSS/JavaScript (Vanilla, keine Dependencies)
- **ML Framework**: MLX (Apple Silicon optimiert)
- **Database**: SQLite
- **Empfohlene Modelle**: Mistral 7B, Llama 3 8B (via MLX)

## Projektstruktur

```
mail-finetuning/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI App
â”‚   â”œâ”€â”€ mail_parser.py       # Mail Import & Bereinigung
â”‚   â”œâ”€â”€ data_manager.py      # SQLite Operationen
â”‚   â”œâ”€â”€ training.py          # MLX Training Wrapper
â”‚   â””â”€â”€ inference.py         # Modell-Inferenz
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ app.js
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mails.db             # SQLite Datenbank
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â””â”€â”€ val.jsonl
â”œâ”€â”€ models/                  # Heruntergeladene Modelle
â”œâ”€â”€ output/                  # Trainierte Adapter
â””â”€â”€ requirements.txt
```

## Installation

### Voraussetzungen

- macOS mit Apple Silicon (M1/M2/M3/M4)
- Python 3.10 oder hÃ¶her
- mindestens 16GB RAM (24GB empfohlen)

### 1. Repository Setup

```bash
cd training
```

### 2. Virtual Environment erstellen

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Dependencies installieren

```bash
pip install -r requirements.txt
```

### 4. Modell herunterladen

WÃ¤hle ein MLX-optimiertes Modell von Hugging Face:

```bash
# Mistral 7B (4-bit quantisiert, ~4GB)
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --local-dir models/Mistral-7B-Instruct-v0.3-4bit

# ODER Llama 3 8B (4-bit quantisiert, ~5GB)
huggingface-cli download mlx-community/Meta-Llama-3-8B-Instruct-4bit \
    --local-dir models/Meta-Llama-3-8B-Instruct-4bit
```

**Hinweis**: Die 4-bit Versionen sind fÃ¼r 24GB RAM optimal. FÃ¼r mehr RAM kÃ¶nnen auch grÃ¶ÃŸere Versionen genutzt werden.

## Nutzung

### 1. Server starten

```bash
cd backend
python main.py
```

Die App ist dann verfÃ¼gbar unter: **http://localhost:8000**

### 2. Workflow

#### Schritt 1: Mails importieren

1. Gehe zu "Mail Import"
2. Ziehe .eml, .mbox oder .txt Dateien per Drag & Drop in den Upload-Bereich
3. Die Mails werden automatisch geparst und bereinigt

#### Schritt 2: Mails labeln

1. Wechsle zu "Labeling"
2. FÃ¼r jede Mail:
   - WÃ¤hle den **Aufgabentyp** (Zusammenfassen, Antwort schreiben, etc.)
   - Gib den **erwarteten Output** ein
   - Klicke "Speichern" oder nutze Shortcut `S`
3. Nutze Shortcuts: `N` (NÃ¤chste), `S` (Speichern), `K` (Skip)

**Tipp**: Mindestens 50 gelabelte Beispiele fÃ¼r gutes Fine-Tuning!

#### Schritt 3: Daten exportieren

1. Gehe zu "Export & Stats"
2. PrÃ¼fe die Statistiken (mind. 50 gelabelte Mails empfohlen)
3. Klicke "JSONL generieren"
4. Optional: Download der JSONL-Dateien zur Archivierung

#### Schritt 4: Training starten

1. Wechsle zu "Training"
2. Konfiguriere Parameter:
   - **Modell**: WÃ¤hle heruntergeladenes Modell
   - **Learning Rate**: Standard 1e-5 (bei Overfitting niedriger)
   - **Epochs**: 3-5 fÃ¼r erste Versuche
   - **Batch Size**: 4 (bei 24GB RAM sicher)
   - **LoRA Rank**: 8-16 (hÃ¶her = mehr KapazitÃ¤t, mehr RAM)
3. Klicke "Training starten"
4. Beobachte Live-Updates:
   - Training/Validation Loss
   - Fortschritt und ETA
   - Speichernutzung

**Warnung bei Overfitting**: Wenn Validation Loss steigt wÃ¤hrend Training Loss sinkt, Training abbrechen!

#### Schritt 5: Modell testen

1. Gehe zu "Evaluation"
2. WÃ¤hle Task-Type und gib Mail-Text ein
3. Klicke "Vergleich starten"
4. Sieh dir die Ausgaben von Base- und Fine-tuned-Modell an

### 3. Export des fertigen Modells

Nach erfolgreichem Training liegen die LoRA-Adapter in `output/run_[timestamp]/adapters.npz`.

Um das Modell zu nutzen:

```python
from mlx_lm import load

model = load(
    "models/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="output/run_1234567890/adapters.npz"
)
```

## API Endpoints

### Mails

- `POST /api/mails/upload` - Mails hochladen
- `GET /api/mails` - Alle Mails abrufen
- `GET /api/mails/{id}` - Einzelne Mail
- `PUT /api/mails/{id}` - Mail aktualisieren (Labeling)
- `DELETE /api/mails/{id}` - Mail lÃ¶schen

### Export

- `GET /api/export/stats` - Statistiken
- `POST /api/export/jsonl` - Training-Daten generieren
- `GET /api/export/download/{train|val}` - JSONL herunterladen

### Modelle

- `GET /api/models` - VerfÃ¼gbare Modelle
- `POST /api/models/download` - Modell herunterladen (Placeholder)

### Training

- `POST /api/training/start` - Training starten
- `POST /api/training/stop` - Training stoppen
- `GET /api/training/status` - Status abrufen
- `GET /api/training/stream` - SSE Stream fÃ¼r Live-Updates

### Inference

- `POST /api/inference/load` - Modell laden
- `GET /api/inference/loaded` - Geladene Modelle
- `POST /api/inference/generate` - Text generieren
- `POST /api/inference/compare` - Modell-Vergleich
- `GET /api/inference/test-prompts` - Test-Prompts

## Tipps & Best Practices

### DatenqualitÃ¤t

- **Mindestens 50 Beispiele** pro Task-Type
- **Einheitlicher Output-Stil**: Achte auf konsistente Formatierung
- **Diverse Beispiele**: Verschiedene Mail-LÃ¤ngen und Stile
- **Klare Labels**: Vermeide mehrdeutige oder widersprÃ¼chliche Labels

### Training

- **Learning Rate**:
  - 1e-5 fÃ¼r die meisten FÃ¤lle
  - 5e-6 bei Overfitting
  - 1e-4 bei sehr kleinem Datensatz (Vorsicht!)

- **Epochs**:
  - 3 Epochs fÃ¼r Start
  - Mehr Epochs wenn Loss noch sinkt
  - Weniger wenn Overfitting auftritt

- **LoRA Rank**:
  - 8 fÃ¼r einfache Tasks
  - 16-32 fÃ¼r komplexe Tasks
  - HÃ¶her = mehr KapazitÃ¤t aber mehr RAM

### Overfitting erkennen

Zeichen von Overfitting:
- âœ… Training Loss sinkt kontinuierlich
- âŒ Validation Loss steigt oder stagniert
- âŒ Modell "memoriert" exakte Trainingsbeispiele

LÃ¶sungen:
- Mehr Daten sammeln
- Kleinere Learning Rate
- Weniger Epochs
- Niedrigere LoRA Rank

## Troubleshooting

### "Out of Memory" Fehler

- Reduziere Batch Size (4 â†’ 2 â†’ 1)
- Nutze kleineres Modell (4-bit quantisiert)
- SchlieÃŸe andere Programme

### Training sehr langsam

- PrÃ¼fe ob Metal Performance Shaders aktiv sind
- Nutze 4-bit quantisierte Modelle
- Reduziere max_seq_length (Standard: 2048)

### Modell gibt schlechte Ergebnisse

- Mehr/bessere Trainingsdaten
- LÃ¤ngeres Training (mehr Epochs)
- HÃ¶here LoRA Rank
- PrÃ¼fe Prompt-Format

## Wichtige Hinweise

### MLX Training Loop

**WICHTIG**: Die aktuelle Implementierung in `training.py` enthÃ¤lt eine **simulierte Training Loop**. FÃ¼r produktiven Einsatz muss diese durch echtes MLX Training ersetzt werden:

```python
# Beispiel fÃ¼r echtes MLX Training mit mlx-lm
from mlx_lm.tuner import train

train(
    model_path=str(model_path),
    data_path=str(train_file),
    val_data_path=str(val_file),
    adapter_file=str(output_path / 'adapters.npz'),
    iters=total_steps,
    learning_rate=config.learning_rate,
    batch_size=config.batch_size,
    # ... weitere Parameter
)
```

Siehe [mlx-lm Dokumentation](https://github.com/ml-explore/mlx-examples/tree/main/llms) fÃ¼r Details.

### Inference

Die Inference-Implementation in `inference.py` nutzt `mlx_lm.generate()`. Stelle sicher, dass das richtige Prompt-Format fÃ¼r dein Modell genutzt wird (z.B. ChatML, Llama-Format, etc.).

## Entwicklung

### Debug-Modus

```bash
uvicorn main:app --reload --log-level debug
```

### Tests (TODO)

```bash
pytest tests/
```

## Lizenz

MIT License

## Support

Bei Problemen:
1. PrÃ¼fe die Browser Console (F12) fÃ¼r Frontend-Fehler
2. PrÃ¼fe die Server-Logs fÃ¼r Backend-Fehler
3. Stelle sicher, dass alle Dependencies installiert sind
4. PrÃ¼fe, dass MLX korrekt auf Apple Silicon lÃ¤uft

## Roadmap

- [ ] Echte MLX Training Loop implementieren
- [ ] Automatisches Checkpoint-Management
- [ ] Model Merging (Base + Adapter zusammenfÃ¼hren)
- [ ] Export fÃ¼r Deployment
- [ ] Batch-Inference
- [ ] Tests
- [ ] Docker Support

---

**Viel Erfolg beim Fine-Tuning! ğŸš€**
